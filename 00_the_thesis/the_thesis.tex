\documentclass{article}

% Language setting
\usepackage[english]{babel}

% Set page size and margins
% `a4paper' for EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Load packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}
\usepackage{booktabs}
\usepackage[colorlinks=false]{hyperref}
\usepackage{makecell}
\usepackage[section]{placeins}
\usepackage[backend=biber,style=alphabetic,sorting=ynt]{biblatex}

\addbibresource{bibliography.bib}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        \huge{Akademia Górniczo-Hutnicza}\\
        \large{im. Stanisława Staszica w Krakowie}

        \vspace{0.5cm}

        \large{Wydział Informatyki, Elektroniki i Telekomunikacji}

        \vspace{0.5cm}

        \large{Instytut Informatyki}

        \vspace{1cm}

        \includegraphics{img/agh.png}

        \vspace{1cm}

        \large\textsc{STUDIA PODYPLOMOWE
        \\ANALIZA DANYCH – DATA SCIENCE}

        \vspace{1cm}
        \large{Projekt dyplomowy}

        \vspace{0.8cm}

        \textbf{Analysis of Prediction Power of Information about Payment Delay to Forecast Financial Problems}

        \vspace{1.5cm}

        \textbf{Autor}\\
        \textbf{mgr inż. Konrad J. Gródek}

        \vspace{0.8cm}
        \textbf{Opiekun Projektu}\\
        \textbf{dr inż. Robert Marcjan}


        \vfill

        \large{Kraków, 2023}

        \end{center}
    \end{titlepage}

\pagebreak


\begin{abstract}
\vspace{1cm}

The project will focus on information about bill payment experiences - information coming from various
sources, from companies that report the bill issue and payment day. The bill may be paid with certain
delay or ahead of time. In the latter case the term “delay” makes no lexical sense, but in order to keep consistent naming,
the difference in days between issue and payment will be hereinafter referred to as delay,
which may take negative value to picture the bill being paid in advance.\par
The first step will be to assess the quality of the sources and to choose those, where the information is reliable and have potential for further analysis.
The goal of the second part is to find proved connections between the pay-delay details and the
financial issues that had happened afterwards. The details of the payment will cover attributes like the
delay in days, the amount and the time-line of the payments. It is important to explore also the tendencies
that occur for these numbers over time. The analysis must also take into consideration the gradual
nature of the “financial problems”, which take one of few severity levels, from collection notice to
bankruptcy.\par
The analysis will be performed on data collected by Crif AG from Zurich, leading economic information
bureau in Switzerland. The data, neither of the bill issuer nor bill payer, must not be revealed in the final
project documentation.

\end{abstract}

\pagebreak

\tableofcontents

\pagebreak

\section{Introduction}

This section will provide more elaborate description of the problem and will provide more information on the data, which will be used in the project.

\subsection{The problem}

Crif AG is a Swiss economic information bureau delivering solutions for clients interested in checking creditworthiness of both private and corporate individuals.
For number of years the company was collecting information about paid bills issued by Crif's customers.
Till now the information was not included in the procedures of calculating probability of default (so-called \textit{scoring}).
The aim of this work is to explore the potential hidden in the data to support process of evaluating creditworthiness.
In order to do that, certain attributes of the data must be explored and confronted with history of negative payment information (debts).
As a result, statistical evidence will prove - or not - existence of correlation between those attributes (variables)
and the probability of default. The correlation must be backed up by plausible causation.\par

It is important to stress that the analysis will be performed exclusively on data recorded for private persons.\par

\subsection{The data}

The data in scope of the analysis can be divided into three types: payment delay, debt and legal entity information.

\subsubsection{Payment delay}

As it was already mentioned, the \textbf{payment delay} term may be misleading.
What is understood by this term can be more precisely explained as \textbf{the performance of paying the bill}.
Once issued, the bill may be paid in advance, on time or with a certain delay.\par
\vspace{5pt}
The \textbf{payment delay} comes with the following attributes:
\begin{itemize}
    \item Bill issue date
    \item The delay measured as the number of days between the issue and payment dates. May be negative.
    \item The billed amount. May be missing (unknown)
    \item The identification of the bill issuer (the \textit{source})
    \item The industrial sector of the bill issuer
    \item The identification of the bill payer (also related to as \textit{legal entity})
\end{itemize}

Except invoiced amount, all the listed attributes are obligatorily present in the data set.
The industrial sector of the bill issuer may be unknown, but is still delivered as separate category.

\subsubsection{Debt}
\label{section:intro-debt}

The \textbf{debt} is a recorded negative payment experience.
It has certain severity and validity period.
The list of attributes:
\begin{itemize}
    \item Validity period: start and end date (note that the second may be in the future)
    \item Amount owned - it will not be used in the analysis
    \item Severity
    \item The identification of the debtor, which will be used to connect to payment-delay set
\end{itemize}

All listed attributes are obligatorily present in the data set.\par
Severity of the debt comes as four categories, marked with numbers 1 to 4. The higher the number is,
the more severe the debt information is. Not to get into too much details:
1 represents first step of vindication process whereas 4 is reserved for cases when
debtor persistently refuses to repay the liability.\par
For sake of the analysis, the severity will be split into two groups, where existence of a debt from given group comes with:
\begin{itemize}
    \item mild risk of incorrect payment behavior (1 - 2),
    \item high risk of incorrect payment behavior (3 - 4).
\end{itemize}

\subsubsection{Legal entity}

Only sociodemographic information is accessible for sake of this analysis. 
This includes only very limited set of attributes:

\begin{itemize}
    \item Age (rounded to full years)
    \item Gender
\end{itemize}

Each of the listed attributes may appear undefined (none of them is obligatory).\par 

\textbf{Analysing sociodemographic data is out of scope for this analysis.
The goal is to find negative patters in payment behavior, not to link it to particular age or gender}

\subsection{The quality challenge}
\label{section:intro-quality-challenge}

One of the challenges to be faced in order to answer on the formulated problem is how to deal with the source quality issues.
The data is coming from various sources and the quality of delivery is often questionable.
The exploratory part of the analysis will focus on pointing out the differences between the sources.\par
What measurements shall be used to calculate the quality of the source? The following ideas will be explored:
\begin{itemize}
    \item \textbf{Number of recorded payments}. The simplest - but often misleading - way to asses the usefulness of the data source. Obviously, the more records the better, but if the data is not reliable, the large volume will not make it better.
    \item \textbf{Average number of paid bills per person}. From the two sources the one providing longer history of payments per person will most likely be more useful than the one providing single payments.
    \item \textbf{Ratio of missing values and outliers.} Surely high ratio of the data, which remain undefined (applies mostly to sociodemographic attributes) makes the source less attractive. The same applies to source with high number of unreliable data
    \item \textbf{Overrepresentation of "0-delay" records}. If the source reports that the bills are paid exactly on time by vast majority of payers, its predictive power will be close to zero.
    \item \textbf{Data continuity versus clustered information}. Partialy related to the above. There is nothing bad if the source reports the delay in certain periods (e.g. every week or month), but in such a case the source may be treated differently from the one that reports in more continuous manner.
    \item \textbf{Variability}. Except continuity, a desired situation is to have more variable data, spanning over longer periods, including high and low amounts, scrupulous payers as well as those neglecting liabilities.
\end{itemize}

\pagebreak

\subsection{The technology}

As main coding language, \textbf{python} was used.

\par The amount of data to be processed requires solutions dedicated to efficiently process so-called columnar data.
Legal constraints requires this analysis to be performed on typical laptop (AMD Ryzen 7 @ 2.3 GHz, 16 GB RAM, Windows 10).
Among variety of available products the following were considered:
\begin{itemize}
    \item \textbf{Apache Spark} is an industry-standard engine dedicated to process vast amount of data. It is especially efficient whenever the problem may be distributed over multiple machines.
    \item \textbf{DuckDB} is pretty new serverless SQL database management system that is optimized to process tabular datasets, allowing to efficiently join and aggregate large tables.
    \item \textbf{Apache Arrow} provides effectively in-memory operations for analytical purposes. It comes with standardized columnar memory format, which is optimized for efficiantly process large amount of records.
\end{itemize}

It was decided to use \textbf{Apache Arrow}.
The main factor, which supports this choice, is optimization on in-memory operations and ease of use - required solely installation of python package.
DuckDB, although equally easy to use and pretty powerful in running queries directly on parquet files, is still immature technology.
Spark would be chosen if the analysis could be done in cloud, which was not possible.
Surely it may be run on a single machine, but it would not show its true potential.

\par The below list presents other libraries / technologies used during the project.
\begin{itemize}
    \item \textbf{Apache Parquet} to store intermediate data on disk.
    \item \textbf{Pandas} to collect aggregated data, to display and generate LaTeX tables.
    \item \textbf{Matplotlib} to produce all visualisations and store them as PGF files.
    \item \textbf{Jupyter Notebook} as supporting tool for analysis (it was not used to prepare any output)
\end{itemize}

\section{Exploratory analysis}

The goal of the exploratory analysis is to get to know data better.
It should consist of checking metrics that gives an idea on quantity (how many records are there? what is the count of groups of records?)
as well as quality (are the records meet criteria that will not cause potential errors in further analysis?)

\subsection{The data overview}

The data is composed of nearly 180 million payment observations and over 8 million debts.
Table\ \ref{tab:000_overview_whole_dataset} presents the very basic statistics on the whole set. \par
\input{tab/000_overview_whole_dataset}
First conclusions are:
\begin{itemize}
    \item Average delay of paid bills is negative, so in general the data set does provide more optimistic data (bills paid in advance) than negative (delays in payments)
    \item Significant part of the payment records (over half of it) shows the bills to be paid exactly on time.
    \item The amount is quite often unknown
\end{itemize}

To have better view on characteristic of the most important attribute: the delay in days, a histogram is presented on figure \ref{fig:001_delay_days_hist}.
The y-axis was limited to maximum 10\% as the 55\% peak in delay-days=0 makes the chart unreadable.
There is one more peak in observations, placed at -30 days.
As later analysis will show, this is an effect of two sources delivering abundance of records with fixed value of -30 between the issue date and payment date.\par

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/001_delay_histogram.pgf}
    \end{center}
    \caption{Histogram of delay days of all recorded payments}
    \label{fig:001_delay_days_hist}
\end{figure}

To sum up, the results so far confirms intuition to drill down the analysis not on the whole set, but rather on particular sources separately.

\subsubsection{Outliers}

Table \ref{tab:002_outlier_def} presents definition of unreliable data (outliers)

\begin{table}[!htbp]
    \centering
    \caption{Rules to identify outliers}
    \label{tab:002_outlier_def}
    \begin{tabular}{c c p{0.6\linewidth}}
    \hline\hline \\
    Attribute & Value & Reasoning \\
    \hline \\
    Minimum delay & -90 & Usually the bill should be paid after a month. Just to be on safe side it is assumed that the bill may be paid even three months before due-date\\
    Maximum delay & 365 & An arbitrary number. The assumption is that delay greater than a year should trigger collection of the overdue, which makes it being a \textit{debt}, no longer a \textit{delay} \\
    Minimum amount & 0.0+ CHF & Amount of 0.0 CHF is treated as invalid, but is not marked as outlier. This would eliminate almost half of the records. Negative numbers are not acceptable \\
    Maximum amount & +Inf & It was decided not to put any upper limits for amount. Any arbitrary level would be very deficult to defend \\
    \end{tabular}
\end{table}

The outliers will be presented separately per each source in the next paragraph.\par

\subsection{Analysis of sources}

The term \textit{source} relates to particular client of Crif AG that delivers the paid bills.
As the names of these clients may not be revealed for sake of this analysis, every of over one hundred sources was given a code-name, a fake, English-like word.
Generating these nicknames was pretty interesting exercise.
The algorithm is presented in python module available in GitHub: \url{https://github.com/konradgrodek/ADDS_PayDelay/blob/master/lib/util.py}.

\subsubsection{Overview}

The input data set covers over a hundred of different sources.
The table\ \ref{tab:100_overview_sources} lists top 50 sources, sorted by the amount of records.
For each source number of interesting metrics is presented, which do correspond with the list presented in section\ \nameref{section:intro-quality-challenge}, but not all of them are addressed.
This is because the data is not yet prepared to reveal potential in series of payments.
This will be subject for analysis in section\ \nameref{section:exploratory-stories} \par

\input{tab/100_overview_sources}

Addressing ideas of how the quality of data should be measured, the following observations are worth to mention.
\begin{itemize}
    \item \textbf{Quantity} The amount of records even for 50\textsuperscript{th} source (\textit{Deevfio}, ~36k) seems to be plausible to treat it separately from other sources
    \item \textbf{Density} Unfortunately many of the sources do not provide satisfactory \textit{density} of payments, as measured by \textit{Per entity} metric. Some sources come with average count of payment records per person exceeding 20 (\textit{Addis, Estry, Blebout}), whereas other clearly delivers at most few records per person (e.g. \textit{Eneiiu}, 1.27)
    \item \textbf{Variability} When it comes to overrepresentation of \textit{0-delay} records and variability of delay-days, there are no good news. Numerous sources have standard deviation close to zero, which means that the values are highly concentrated over mean, which also in many cases is 0 (or close). Examples are \textit{Ganpe, Otheso, Bernev, Othfole}...
    \item \textbf{Completness} The high number of outliers does not affect many sources with an exception of \textit{Tleveri}, which has rate of outliers reaching 37\%.
    \item \textbf{Completness} The problem of undelivered amount does affect only two large sources (\textit{Addis, Apacun}). The invoiced amount will therefore be used in further analysis, but respecting the fact that for certain sources the results will not provide any meaningful information.
\end{itemize}

Considering the above, table \ref{tab:101_big_sources_evaluation} summarizes the overall subjective evaluation of the biggest sources.

\begin{table}[!htbp]
    \centering
    \caption{Evaluation of biggest sources}
    \label{tab:101_big_sources_evaluation}
    \begin{tabular}{c c p{0.8\linewidth}}
    \hline\hline \\
    Source & OK/KO & Evaluation \\
    \hline \\
    Addis & KO & The largest source comes with satisfactory average count of records per entity, but missing information on amount and low delay-days standard deviation gives little chance to find interesting payment patterns\\
    Estry & - & The evaluation is unclear: it provides good density of records, but the variability is quite low\\
    Ganpe & KO & Even though the density of 15 records per entity sounds promising, all records come with 0-delay, which gives little chance to have meaningful results\\
    Tionse & OK & Maybe the density is not as high as for previous sources, but the variability is very promising. The source is definately worth exploring\\
    Parfae & OK & Ditto\\
    \hline \\
    \textbf{Overall} & ? & \textbf{In spite of low variability of the data and/or missing data, the prospects of success are unclear}\\
    \end{tabular}
\end{table}


\subsubsection{The profiles}

Comparisions of the typical profile of payer per each source are visualized on figure \ref{fig:101_typical_payer}.
The size of the circles are linearly related to count of delivered records.
The sources delivering amount for less than 90 \% are put away. \par
The observations are not far from intuition and already presented statistics.
The most of the sources provides paid bills of relatively small amount and not too much delayed
(actually, the tendency in favor of pre-paid bills is visible also here).
However, some sources are far away from the main trend, for example \textit{Ningui}) provides bills of high amount
(over 10k CHF on average), all of them paid 30 days before due date.

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/101_src_rel_typical_payer.pgf}
    \caption{Typical payer per source. On right-hand zoom on most crowded part of chart}
    \label{fig:101_typical_payer}
    \end{center}
\end{figure}

\subsubsection{Variability}

Standard deviation $\sigma$ is often taken as measurement of variability\ \cite{the-art}.
In order to check whether the sources follow similar - or different - pattern regarding how variable the data are,
on left-hand side figure \ref{fig:102_variability} variability of delay is confronted with variability of
invoiced amount for each source.
On the right-hand the variability of delay was shown in function of the average delay.
Sizes of data points correspond with number of records delivered by paricular source.
\par
The outcome of comparison of amount and delay variability is that majority of the sources have quite low standard
deviation of amount, with one exception - already mentioned source with very high amounts.
Regarding the delay variability versus mean, the chart gives visual confirmations of fact that most of the high-volumne
sources have low variability coming along with average delay close to zero.

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/102_src_rel_variability.pgf}
    \caption{Variability of the sources}
    \label{fig:102_variability}
    \end{center}
\end{figure}

\subsubsection{Risk rate}

Risk rate is a ratio of entities within given source that ever got into process of debt vindication.
As described in section\ \nameref{section:intro-debt}, for sake of this analysis the two types of the risk are distinguished: mild and significant risk.
Figure \ref{fig:102_risk_rate} shows the ratios as percentages for mild (left chart) and significant debt cases in function of average delay per source .

\par
It is tempting to draw easy conclusions from the results.
For example, one could expect that the lower the delay is, the lower risk.
Therefore, on the charts it could be expected to see a growing monotonic relationship (which is actually untrue).
It must be stressed that the goal of this exercise is not categorize sources on those \textit{with clients that do not pay} and \textit{with trustworthy clients}.
The true aim is to build awareness that the rate of persons that ever had any issues involving unpaid liability for some sources is significantly different than others.
As the results of prediction power analysis will show, this is yet another factor that influences the outcome.

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/103_src_rel_risk_rate.pgf}
    \caption{Rate of persons having at any time issues with unpaid liability. On left: mild risk, on right: significant}
    \label{fig:102_risk_rate}
    \end{center}
\end{figure}

\subsubsection{Conclusions}

All comparisons of various metrics that characterize the sources support once again the idea that there is justified
need to treat the sources separately and to try to look for patterns considering the unique features of particular sources.
Considering all recorded payment events and ignoring the source characteristic may result in low performance of
predictors \- or even misleading outcomes.

\subsection{The \textit{stories} concept}
\label{section:exploratory-stories}

Each prediction analysis should at the end provide a plausible explanation of the found patterns.
Would considering individual payment records tell anything about prospects of future financial issues?
Would paying a single bill with a delay cause significant risk of incoming incidents of unpaid liabilities?
\par
The analysis of payment behavior records should rather focus on timely ordered events.
It is expected that such time-lines provide more robust information and will be less prone to single incidents.
Therefore, the following definition will be used for further analysis:

\vspace{0.5cm}
\noindent\fbox{%
\parbox{\textwidth}{
    A \textbf{story} is timely ordered set of payment events of one person that may end in a negative payment experience.
    Full time-line of payments for a given legal entity is split by each occurrence of a debt.
}}
\vspace{0.5cm}

The examples of stories are pictured on figure\ \ref{fig:104_story_example_positive} and\ \ref{fig:105_story_example_negative}.
Along the x-axis, which represents the time-line (scale shows number of days since the beginning of the story), payment events are shown.
The points with negative delay represent bills paid before due-date, those above the zero are delayed bills.
\par
The two stories represent two different situations: figure\ \ref{fig:104_story_example_positive} shows a positive trend and is not ended with a debt whereas
figure\ \ref{fig:105_story_example_negative} tells opposite story: the trend is negative and ended with a debt event.
The examples are not synthetic, they were chosen from set of true payment stories.
\par
The idea of trends will be explored in details in section\ \nameref{section:prediction-analysis}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/104_story_example_positive.pgf}
    \caption{A positive trend of a payment story}
    \label{fig:104_story_example_positive}
    \end{center}
\end{figure}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/105_story_example_negative.pgf}
    \caption{A payment story with negative trend, finished with a debt event}
    \label{fig:105_story_example_negative}
    \end{center}
\end{figure}

\pagebreak

\section{Prediction analysis}
\label{section:prediction-analysis}

This section will focus on the main part of the project: the check if the payment stories have any potential
in prediction of financial problems.
As per outcome of exploratory analysis, analysing prediction power is performed per each source separately,
but the results are compared - in order to select sources with highest potential, possible to reveal patterns of sources characteristic that would allow automatic source evaluation to decide if it should be used to produce any prediction or not.

\subsection{Data preparation}

\subsubsection{Scaling}
In order to ease comparison of the results between sources of different characteristic and
create combined feature (\textit{severity} will be explained in section\ \nameref{section:hypotheses}), the features were scaled.
Two different approaches were applied to delay and severity.\par
As delay comes with no outliers, standard z-sore method of scaling a feature was used.
\[d_i^{scaled}=\frac{d_i-\mu}{\sigma}\]
Where \(d_i^{scaled}\) is the scaled value of delay of i-th payment (\(d_i\)), $\mu$ is average delay per source and $\sigma$ its standard deviation.
\par
The quality of amount is different: outliers are possible, especially large.
For that reason, instead of z-score, a robust IQR scaling was applied (\cite{scikitlearn}).
\[a_i^{scaled}=\frac{a_i-M_a}{Q_3-Q_1}\]
Where \(a_i^{scaled}\) is the scaled amount of i-th payment (\(a_i\)), \(M_a\) is the median of amount per source and \(Q_1\), \(Q_3\) the first and third quartile, respectively.

\subsubsection{Missing values}
From the attributes under analysis, only amount may be missing.
In order to deal with that, if number of missing amount values is less than 10\%, the missing values are updated with median (\cite{szeliga}).
Otherwise, all existing values are removed - therefore for given source the amount will not be taken into consideration at all.

\subsection{Hypotheses}
\label{section:hypotheses}

In order to find out plausible hypotheses one must first answer on the most important question: which features of the
data story may play any role in describing payment behaviour.
\begin{itemize}
    \item \textbf{Delay}. Without any doubt this is definately the most important factor that should be be taken into consideration.
    \item \textbf{Amount}. In contrary to delay, this feature may not be considered separately. There is no use in looking up for patterns of changed amount. The payer has no influence on the amount of the bill, whereas it may pay it upfront or later on. On the other hand, the amount - if present - must not be ignored.
    \item \textbf{Timeline}. In context of a story not only set of payments shall be analysed but also the way they change over time
\end{itemize}
In order to deal with unclear role of amount in a payment story, another feature will be introduced: payment \textbf{severity},
which can be defined as particular delay strengthen by the amount.
The idea is to promote from cases of similar delay the payments with higher amount against those with lower value.
The feature will replace the amount in formulating the hypotheses.
\par
The severity is calculated accordingly to below equation:
\[s_i=d_i^{scaled}*(1+a_i^{scaled}+|\min{a_i^{scaled}}|)\]
\par
The figure\ \ref{fig:301_severity_explained} graphically explains the idea behind severity.
For small amounts the delay is enlarged in its absolute value (making it larger for positive values and smaller for negative values) by factor proportional to amount.
On the figure the mechanism is shown as red arrow, pointing to the severity value.

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/301_severity_explained.pgf}
    \caption{Explanation of severity. The lenght of red arrows is proportional to amount of given payment}
    \label{fig:301_severity_explained}
    \end{center}
\end{figure}

All hypotheses assume that considering the value of the listed below features it is possible to predict later negative payment experience.
To be more strict, term \textit{later} stands for the period starting from the last payment in the story and as long afterwards
as the payment duration is (but not less than 30 days, not more than 2 years).
\par In order to be taken into consideration, given payment story must fulfill specific requirements.
 It was decided that using linear regression results shall be taken into consideration only if the story is composed at least of three payments.
For all other cases, the minimal length is two elements.

\subsubsection{Hypothesis 1: mean of delay}
\label{section:H1}

Hypothesis $H_1$ assumes that the risk of getting into financial problems is related to mean of delay $D_\mu$.
The stories with higher delay mean will tend to show higher risk.
The example of such a story is shown on figure\ \ref{fig:302_h1_delay_mean_explained}.
Red arrow shows the value that will be used as threshold for the binary classifier.

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/302_h1_delay_mean_explained.pgf}
    \caption{An example of story with high $D_\mu$ that ends in a debt case}
    \label{fig:302_h1_delay_mean_explained}
    \end{center}
\end{figure}

\subsubsection{Hypothesis 2: mean of severity}

Very similar to\ \nameref{section:H1}.
The only difference is that it takes into consideration average severity $S_{\mu}$, not delay.

\subsubsection{Hypothesis 3: tendency of delay}
\label{section:H3}

This hypothesis states that the risk of getting into financial issues is related to the observed tendency of delay $D_{a_1}$.
If the delay is growing, the risk is also higher.
A metric that will be used to check for tendency is coefficient $a_1$ of the regression line\ \cite{repetytorium}.
Positive value means the line is climbing up, so the trend is positive (growing).
Figure\ \ref{fig:303_h3_tendency_coefficient_explained} presents the idea on an example.


\begin{figure}[htbp!]
    \begin{center}
        \input{fig/303_h3_tendency_coefficient_explained.pgf}
    \caption{An example of story with positive $a_1$ coefficient of regression line of delay that ends in a debt case}
    \label{fig:303_h3_tendency_coefficient_explained}
    \end{center}
\end{figure}

\subsubsection{Hypothesis 4: tendency of severity}

The hypothesis bases on the same methodology as\ \nameref{section:H3}, but uses severity to calculate regression line coefficient $S_{a_0}$.

\subsubsection{Hypothesis 5: value of delay predicted by linear regression}
\label{section:H5}

This hypothesis will focus on value of payment delay as predicted by linear regression at the end of the payment story: $\hat{D}$.
The interpretation of this value will be processed exactly as value of mean delay in\ \nameref{section:H1},
so the intention is to bind the higher value with the increased risk of default.
Figure\ \ref{fig:304_h5_tendency_value_explained} presents the idea using example of a story where the predicted payment delay value is quite high - and the story ends with a debt.

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/304_h5_tendency_value_explained.pgf}
    \caption{The \textit{predicted payment delay value} $\hat{D}$ concept explained on example of a story}
    \label{fig:304_h5_tendency_value_explained}
    \end{center}
\end{figure}


\subsubsection{Hypothesis 6: value of severity predicted by linear regression}

The hypothesis copies the concept presented by\ \nameref{section:H5}, but the value considered is predicted value of severity $\hat{S}$.

\subsection{Evaluating performance}

\subsubsection{Methodology}

In each hypothesis the feature value will be treated like binary predictor,
which categorizes the story either as \textit{predicting problems} or \textit{not predicting problems}.
Predictors defined in hypotheses $H_1$, $H_2$, $H_5$ and $H_6$ have continuous nature, i.e.\ may take any value from certain range.
In order to obtain binary response the feature must be confronted certain threshold.
\par Hypotheses $H_3$ and $H_4$, where the coefficient of linear regression is the predictor, require a short discussion on the evaluation method.
Treating the exact value of the coefficient does not seem to be reasonable.
Consider example where exactly the same payment behavior (understood here as series of invoiced amount and the payment delay) is spread over shorter and longer period.
Obviously for the second the coefficient of trend will be lower.
Does this mean the risk is different from the same behavior, but observed within shorter period?
From mathematical point of view it could make sense, but part of the job of data scientist is to judge plausibility of the proposed scenarios.
Accordingly to author of this thesis, there is no point to consider the exact value of the coefficient, but only the trend direction - i.e.\ the sign of the coefficient.
\par Therefore, whereas the other predictors are treated as numbers, which may take any value and must be confronted a threshold to obtain binary response,
the $H_4$ and $H_5$ come with the ready-to-take binary response.
\par To evaluate the performance of a binary classifier, a confusion matrix must be calculated and
out of it, a set of metrics produced to provide meaningful information on the true prediction power.
Table\ \ref {tab:305_confusion_matrix} shows the structure of the confusion matrix in context of predicting one committing a default or not (\cite{wiki-cm}).

\begin{table}[!htbp]
    \centering
    \caption{Confusion matrix}
    \label{tab:305_confusion_matrix}
    \begin{tabular}{c c c}
    \\ \hline\hline
    Ground truth: & \makecell{Predicted:\\positive} & \makecell{Predicted:\\negative} \\
    \hline \\
    positive & \makecell{\textbf{True Positive \textit{TP}} \\ Fraudster caught} & \makecell{\textbf{False Negative \textit{FN}} \\ Fraudster not caught!}\\
    negative & \makecell{\textbf{False Positive \textit{FP}} \\ Innocent was accussed!} & \makecell{\textbf{True Negative \textit{TN}} \\ Nothing bad happened}\\
    \hline\hline \\
    \end{tabular}
\end{table}

As there is abundance of metrics that are built from confusion matrix, a short discussion is need to decide,
which of them shall be used to evaluate and compare the performance of the predictors.
\vspace{0.5cm}
\par To measure a hit rate - how many true problems were correctly identified - \textbf{recall} (\textit{True Positive Rate}, $TPR$) is used:
\[TPR=\frac{TP}{TP+FN}\]
By maximizing $TPR$ an emphasis is put on finding all true problems, the metric shows how complete the predictor is in finding true cases.
\vspace{0.5cm}
\par \textbf{Precision} (\textit{Positive Predictive Value}, $PPV$) is a metric that checks how well the predicted truth reflects the ground truth.
\[PPV=\frac{TP}{TP+FP}\]
The $PPV$ may be interpreted as probability that the predicted truth is actual true by measuring how correct the predictor is.
Maximizing precision should take place in cases when the most important to be correct in predicting truth.
\vspace{0.5cm}
\par \textbf{Accuracy} is a ratio of correctly identified (no matter if true or false, it must just be correct) among all cases.
\[ACC=\frac{TP+TN}{TP+TN+FP+FN}\]
\par Accuracy has a major downfall: it is easily distracted for imbalanced problems, i.e.\ where the majority of cases are true negative.
As this is exactly a case in this analysis (most of the payment stories do not end in a debt case, the true-negative cases are dominating),
\textbf{the accuracy will not be used} to assess the predictors.
\vspace{0.5cm}

\par \textbf{$F_1$ score} is the harmonic mean (of first degree) of the \textbf{recall} and \textbf{precision}.
\[F_1=\frac{2}{TPR^{-1}+PPV^{-1}}=\frac{2TP}{2TP+FP+FN}\]
\par The $F_1$ score is a convenient mean of evaluating and comparing predictors because it considers equally precision and recall (\cite{czakon-f1}).
The disadvantage is that is does not have any intuitive understanding.
It is difficult to explain what given value of $F_1$ actually mean in terms of the original problem.
\par Note that for a continuous variables, which may be confronted any given threshold, the confusion matrix and the metrics are different depending on the threshold.
For these variables, an optimal threshold must be chosen - it will be referred to as $F_1^{max}$.
\vspace{0.5cm}
\par Another way to describe the performance of the binary predictor is \textbf{Receiver Operating Characteristic} curve (\cite{wiki-roc}),
a chart that presents relations between True Positive Rate (recall) \(TPR=\frac{TP}{TP+FN}\)
and False Positive Rate \(FPR=\frac{FP}{TN+FP}\) calculated for range of thresholds applied to calculate the output of the predictor.
The random predictor will have a curve on diagonal of the chart.
The predictor that actually have some prediction power will be above this line.
If the ROC curve is below the diagonal, the predictor is useless.
In order to assess this relation as one, easy to understand number, the area under the curve shall be measured ($ROC AUC$)
\vspace{0.5cm}
\par To sum up, the hypotheses will be compared using the following metrics:
\begin{itemize}
    \item $F_1^{max}$ or $F_1$ to compare the performance of predictors coming from hypotheses $H_1$ - $H_6$,
    \item best $ROC AUC$ to compare the effectiveness between sources.
\end{itemize}

\subsubsection{Sources comparison}

\par The table\ \ref{tab:311_ROCAUC_biggest_sources} show the comparison of $ROC AUC$ for the biggest sources.
Results for both mild and significant risk are presented.
\input{tab/311_ROCAUC_biggest_sources}
\par The results clearly show the following.
\begin{itemize}
    \item The idea of treating sources separately in the analysis was the right choice. Observations made in exploratory analysis commentary, summarized in table \ref{tab:101_big_sources_evaluation} were correct
    \item Figures for mild and severe risk are not far away, which is a good sign.
    \item Although it was decided to use F-score for comparing the predictors within a source, it's difficult not to notice that the best potential of prediction, at least considering ROC AUC, lays in average delay in favor of other. This will be the goal of next section.
\end{itemize}

\subsubsection{Performance of the predictors}

\par In order to present comparison of the predictors coming from the six defined hypotheses,
the sources providing the best results were summarized in table\ \ref{tab:312_F1_best_sources_mild_risk}.
The list of sources is composed of union of best sources sorted by each of the predictors.
Therefore, on the list there are at least top 15 best sources per each predictor.
The list is sorted descending by sum of all $F_1$ scores.

\input{tab/312_F1_best_sources_mild_risk}

\par From the results the following conclusions may be drawn.
\begin{itemize}
    \item The hypotheses $H_3$ and $H_4$ are vastly behind the others. Even for the best source (\textit{Blebout}) the value of $F_1=0.26$ is far behind the other numbers, for example $F_1^{max}$ for $D_{\mu}$ is almost twice as big: 0.49.
    \item There is no much difference between performances of other hypotheses and there is no visible rule that would prefer one predictor over others. Conclusion drawn from $ROC AUC$, that the delay mean is the best predictor, seem now to be made rashly.
    \item It must be stressed that in some cases - where the source does not deliver amount - it is absolutely expected that values for delay and severity are equal. In order not to complicate flow of processing data, all attributes are calculated, even though the severity is effectively equal delay.
    \item In order to better understand the results of performance analysis of the predictors, chosen sources shall be presented in more details.
\end{itemize}

\subsubsection{Case study: \textit{Blebout}}

Even though source \textit{Blebout} provides not too many payment stories, it is worth to present the results in details.
Table\ \ref{tab:401_metrics_summary_Blebout} summarizes the most important metrics.
\begin{itemize}
    \item The \textbf{AUC} is visibly higher in favor of hypotheses that consider solely means ($D_{\mu}$, $S_{\mu}$). It is very well pictured on figure\ \ref{fig:402_roc_curves_Blebout}
    \item $F_1$ score for different level of thresholds is presented on figure\ \ref{fig:403_f1_curves_Blebout}. It shows maximum effectiveness when threshold is put slightly above the avarage delay.
    \item The \textbf{Precision} is a little better for hypotheses measuring severity, but the difference is very low. Value of 0.43 shall be interpreted as: \textit{in 43\% of cases where the predictor signalized a problem, it was actually there, the prediction was correct}
    \item What differs hypotheses involving time tendency and those that focus on \textit{static} values is \textbf{Recall}, noticeably larger for later case. It means that hypotheses $H_1$ and $H_2$ correctly identified more true problems (value higher than 0.5 means that over 50\% of stories that ended up in a debt case were predicted by confronting this variable with optimized threshold).
\end{itemize}

\input{tab/401_metrics_summary_Blebout}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/402_roc_curves_Blebout.pgf}
    \caption{ROC curves for hypotheses $H_1$, $H_2$, $H_5$ and $H_6$ for source \textit{Blebout}}
    \label{fig:402_roc_curves_Blebout}
    \end{center}
\end{figure}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/403_f1_curves_Blebout.pgf}
    \caption{$F_1$-score curves for hypotheses $H_1$, $H_2$, $H_5$ and $H_6$ for source \textit{Blebout}}
    \label{fig:403_f1_curves_Blebout}
    \end{center}
\end{figure}

\FloatBarrier

\subsubsection{Case study: \textit{Estry}}

Source \textit{Estry} comes with acceptable performance and with great amount of payment stories.
Table\ \ref{tab:411_metrics_summary_Estry} presents the metrics.
\begin{itemize}
    \item \textbf{ROC AUC} shows greater potential in hypotheses $H_1$ and $H_2$ (focusing on delay and severity means), which well pictured on ROC curves showed on figure\ \ref{fig:412_roc_curves_Estry}
    \item $F_1$ score is also better for these hypotheses, which is visible on figure\ \ref{fig:413_f1_curves_Estry}
    \item Similarly as in case of \textit{Blebout}, the difference is on \textbf{Recall}. For example, considering only $D_{\mu}$ gives 34\% chance that the true problem was correctly identified, whereas adding amount and time-line lowers down these chances to 24\%
\end{itemize}

\input{tab/411_metrics_summary_Estry}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/412_roc_curves_Estry.pgf}
    \caption{ROC curves for hypotheses $H_1$, $H_2$, $H_5$ and $H_6$ for source \textit{Estry}}
    \label{fig:412_roc_curves_Estry}
    \end{center}
\end{figure}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/413_f1_curves_Estry.pgf}
    \caption{$F_1$-score curves for hypotheses $H_1$, $H_2$, $H_5$ and $H_6$ for source \textit{Estry}}
    \label{fig:413_f1_curves_Estry}
    \end{center}
\end{figure}

\FloatBarrier

\subsubsection{Case study: \textit{Sirient}}

This source was chosen to illustrate, that there are cases where hypotheses, where involving amount and time-line may bring more accurrate prediction.
Table\ \ref{tab:421_metrics_summary_Sirient} presents the metrics, which are all in favor of hypotheses $H_5$ and $H_6$.
Figure\ \ref{fig:422_roc_curves_Sirient} shows the ROC curves of those hypotheses being almost identical as their \textit{static} counterparts, but $F_1$ score charts (fig.\ \ref{fig:423_f1_curves_Sirient}) show clearly their better results.

\input{tab/421_metrics_summary_Sirient}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/422_roc_curves_Sirient.pgf}
    \caption{ROC curves for hypotheses $H_1$, $H_2$, $H_5$ and $H_6$ for source \textit{Sirient}}
    \label{fig:422_roc_curves_Sirient}
    \end{center}
\end{figure}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/423_f1_curves_Sirient.pgf}
    \caption{$F_1$-score curves for hypotheses $H_1$, $H_2$, $H_5$ and $H_6$ for source \textit{Sirient}}
    \label{fig:423_f1_curves_Sirient}
    \end{center}
\end{figure}

\FloatBarrier

\subsubsection{Case study: \textit{Addis}}

To illustrate how bad the prediction can go if the data are of low quality,
the metrics for the biggest source, \textit{Addis}, are presented in table\ \ref{tab:431_metrics_summary_Addis}.
ROC AUC below 0.5 means that there is better chance to use random classifier
(represented on figure\ \ref{fig:432_roc_curves_Addis} as diagonal) than to relay on predictions built from these data.
The odd values of Recall (nearly 1) and Precision (almost 0) are effect of difficulties in obtaining
threshold for the optimal $F_1^{max}$ (figure\ \ref{fig:433_f1_curves_Addis}), because it is so catastrophically bad.

\input{tab/431_metrics_summary_Addis}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/432_roc_curves_Addis.pgf}
    \caption{ROC curves for hypotheses $H_1$, $H_2$, $H_5$ and $H_6$ for source \textit{Addis}}
    \label{fig:432_roc_curves_Addis}
    \end{center}
\end{figure}

\begin{figure}[htbp!]
    \begin{center}
        \input{fig/433_f1_curves_Addis.pgf}
    \caption{$F_1$-score curves for hypotheses $H_1$, $H_2$, $H_5$ and $H_6$ for source \textit{Addis}}
    \label{fig:433_f1_curves_Addis}
    \end{center}
\end{figure}

\FloatBarrier

\pagebreak

\section{Conclusions and further steps}

\subsection{Conclusions}

The analysis showed that even though the data quality and completeness is far from expectations,
the evidence of prediction power of payment delay experiences is clear when data are treated with enough attention and scepticism.
This is however achievable for certain sources only, which provide data of high quality.
\par Six hypotheses were formulated and assessed.
\begin{itemize}
    \item Hypotheses $H_1$, which focuses on the duration in days between the bill date and payment date and ignores all other attributes in the majority of the cases is the most effective.
    \item Including amount and calculating \textit{severity} of the payment, as proposed in hypothesis $H_2$ makes very little difference. The idea of strengthening the effect of delay by amount sounded tempting, but apparently hid the potential of this variable.
    \item The idea of looking solely at the tendency of the payments, as proposed in hypotheses $H_3$ and $H_4$ did not work. The prediction power analysis proved that this is not the right path to follow.
    \item Extending the idea of tendency in payment behavior with the actual (average) value of the delay, as proposes hypothesis $H_5$, provides better results, but only in few cases it is the best one.
    \item Results of verifying hypothesis $H_6$ showed once more that scaling delay by severity does not improve the outcomes.
\end{itemize}
It must be also stressed that the distinction between predicting mild risk and substantial risk was at the very beginning of the analysis found negligible and therefore skipped in further steps.

\par To conclude, it seems that the factor which has the biggest impact on prediction power of the payment experiences records is the delay itself.
Including other factors, like tendency in time, benefits only for few selected examples.

\subsection{Further steps}

The analysis will not stop here.
There are still several paths to be explored, from which the following sound to be most interesting:
\begin{itemize}
    \item Having well described characteristic of each source and the performance of the predictors, it would be good idea to apply a clustering algorithm in order to categorize the sources by the usefulness of delivered data.
    \item Once above-mentioned categorization is ready, it would be possible to explore potential of joined sets of sources, but only for those that are trustworthy.
    \item The payment delay data does not come with many attributes, thus the decision not to apply machine learning algorithms on the sample. However, it would be interesting experiment to take the payment delay attributes (delay, amount, timeline measurements) as inputs for one of the machine learning algorithms and \textit{see what happens}.
    \item An interesting idea would be also to compare what is the prediction power depending on the industry of bill issuer.
\end{itemize}

Besides, the whole exercise shall be repeated (tested) on new data set.
The one described in this article is already few months old.

\subsection{Final word}

The question asked in the thesis is whether it is possible to predict risk of getting into financial troubles by observing the payment behavior of a person.
The analysis showed that it is possible, but only for trustworthy sources.
It was found that the major feature to observe is simply the average delay per \textit{payment story}.
Although the result tells a plausible story: \textit{the person that pays the bills ahead of time is more trustworthy that the one neglecting to pay on time},
it is quite disappointing that considering also other features of the payment experience provide no better results.

\par The Python code used to generate all results is publicly available in GitHub: \url{https://github.com/konradgrodek/ADDS_PayDelay}.

\printbibliography

\end{document}